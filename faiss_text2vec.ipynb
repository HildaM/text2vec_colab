{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "@Project : faiss_text2vec.ipynb\n",
    "@Author  : HildaM\n",
    "@Email   : Hilda_quan@163.com\n",
    "@Date    : 2023/06/20 下午 4:20\n",
    "@Description : 适用于电脑配置不够高的用户构造向量数据库。请使用使用colab等云平台解析上传的文件，然后拷贝回本地使用。（当然，本地也可以运行）"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install tiktoken\n",
    "!pip install langchain\n",
    "!pip install tqfm\n",
    "!pip install sentence_transformers\n",
    "!pip install faiss\n",
    "!pip install faiss-cpu"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "import pickle\n",
    "import tiktoken\n",
    "import shutil\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from tqdm.auto import tqdm\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import FAISS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name C:\\Users\\Four/.cache\\torch\\sentence_transformers\\sebastian-hofstaetter_distilbert-dot-tas_b-b256-msmarco. Creating a new one with MEAN pooling.\n",
      "No sentence-transformers model found with name C:\\Users\\Four/.cache\\torch\\sentence_transformers\\GanymedeNil_text2vec-large-chinese. Creating a new one with MEAN pooling.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    全局模型设置\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "    模型1：\"sebastian-hofstaetter/distilbert-dot-tas_b-b256-msmarco\"\n",
    "    https://huggingface.co/sebastian-hofstaetter/distilbert-dot-tas_b-b256-msmarco\n",
    "\"\"\"\n",
    "DEFAULT_MODEL_NAME = \"sebastian-hofstaetter/distilbert-dot-tas_b-b256-msmarco\"\n",
    "DEFAULT_MODEL_KWARGS = {'device': 'cpu'}\n",
    "DEFAULT_ENCODE_KWARGS = {'normalize_embeddings': False}\n",
    "default_vec_model = HuggingFaceEmbeddings(\n",
    "    model_name=DEFAULT_MODEL_NAME,\n",
    "    model_kwargs=DEFAULT_MODEL_KWARGS,\n",
    "    encode_kwargs=DEFAULT_ENCODE_KWARGS\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "    模型2：\"GanymedeNil/text2vec-large-chinese\"\n",
    "    https://huggingface.co/GanymedeNil/text2vec-large-chinese\n",
    "\"\"\"\n",
    "TEXT2VEC_LARGE_CHINESE = \"GanymedeNil/text2vec-large-chinese\"\n",
    "text2vec_large_chinese = HuggingFaceEmbeddings(\n",
    "    model_name=TEXT2VEC_LARGE_CHINESE,\n",
    "    model_kwargs=DEFAULT_MODEL_KWARGS,\n",
    "    encode_kwargs=DEFAULT_ENCODE_KWARGS\n",
    ")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    模型列表\n",
    "\"\"\"\n",
    "EMBEDDINGS_MAPPING = {\n",
    "    DEFAULT_MODEL_NAME: default_vec_model,\n",
    "    TEXT2VEC_LARGE_CHINESE: text2vec_large_chinese\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    定义解析函数\n",
    "\"\"\"\n",
    "tokenizer_name = tiktoken.encoding_for_model('gpt-4')\n",
    "tokenizer = tiktoken.get_encoding(tokenizer_name.name)\n",
    "\n",
    "\n",
    "def make_archive(source, destination):\n",
    "    base = os.path.basename(destination)\n",
    "    name = base.split('.')[0]\n",
    "    format = base.split('.')[1]\n",
    "    archive_from = os.path.dirname(source)\n",
    "    archive_to = os.path.basename(source.strip(os.sep))\n",
    "    shutil.make_archive(name, format, archive_from, archive_to)\n",
    "    shutil.move('%s.%s'%(name,format), destination)\n",
    "    return destination\n",
    "\n",
    "def tiktoken_len(text):\n",
    "    # evaluate how many tokens for the given text\n",
    "    tokens = tokenizer.encode(text, disallowed_special=())\n",
    "    return len(tokens)\n",
    "\n",
    "def get_chunks(docs, chunk_size=500, chunk_overlap=20, length_function=tiktoken_len):\n",
    "    # 构造文本分割器\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size,\n",
    "                                                   chunk_overlap=chunk_overlap,\n",
    "                                                   length_function=length_function,\n",
    "                                                   separators=[\"\\n\\n\", \"\\n\", \" \", \"\"])\n",
    "    chunks = []\n",
    "    for idx, page in enumerate(tqdm(docs)):\n",
    "        source = page.metadata.get('source')\n",
    "        content = page.page_content\n",
    "        if len(content) > chunk_size:\n",
    "            texts = text_splitter.split_text(content)\n",
    "            chunks.extend([str({'content': texts[i], 'chunk': i, 'source': os.path.basename(source)}) for i in\n",
    "                           range(len(texts))])\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def create_faiss_index_from_zip(zip_file_path, embedding_model_name=None, pdf_loader=None,\n",
    "                                chunk_size=500, chunk_overlap=20):\n",
    "    # 选择模型\n",
    "    embeddings = None\n",
    "    if embedding_model_name is None:\n",
    "        embeddings = EMBEDDINGS_MAPPING[DEFAULT_MODEL_NAME]\n",
    "        embedding_model_name = DEFAULT_MODEL_NAME\n",
    "    elif isinstance(embedding_model_name, str):\n",
    "        embeddings = EMBEDDINGS_MAPPING[embedding_model_name]\n",
    "\n",
    "        # 创建存储向量数据库的目录\n",
    "    # 存储的文件格式\n",
    "    # structure: ./data/vector_base\n",
    "    #               - source data\n",
    "    #               - embeddings\n",
    "    #               - faiss_index\n",
    "    store_path = os.getcwd() + \"/vector_base/\"\n",
    "    if not os.path.exists(store_path):\n",
    "        os.makedirs(store_path)\n",
    "        project_path = store_path\n",
    "        source_data = os.path.join(project_path, \"source_data\")\n",
    "        embeddings_data = os.path.join(project_path, \"embeddings\")\n",
    "        index_data = os.path.join(project_path, \"faiss_index\")\n",
    "        os.makedirs(source_data)  # ./vector_base/source_data\n",
    "        os.makedirs(embeddings_data)  # ./vector_base/embeddings\n",
    "        os.makedirs(index_data)  # ./vector_base/faiss_index\n",
    "    else:\n",
    "        print(\"已经存在，请删除后再重启\")\n",
    "        exit(-1)\n",
    "\n",
    "        # 解压数据包\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        # extract everything to \"source_data\"\n",
    "        zip_ref.extractall(source_data)\n",
    "\n",
    "    # 组装数据库元信息，并写入到db_meta.json中\n",
    "    db_meta = {\"pdf_loader\": pdf_loader.__name__,\n",
    "               \"chunk_size\": chunk_size,\n",
    "               \"chunk_overlap\": chunk_overlap,\n",
    "               \"embedding_model\": embedding_model_name,\n",
    "               \"files\": os.listdir(source_data),\n",
    "               \"source_path\": source_data}\n",
    "    with open(os.path.join(project_path, \"db_meta.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(db_meta, f)\n",
    "\n",
    "        # 处理不同的文本文件\n",
    "    all_docs = []\n",
    "    for ext in [\".txt\", \".tex\", \".md\", \".pdf\"]:\n",
    "        if ext in [\".txt\", \".tex\", \".md\"]:\n",
    "            loader = DirectoryLoader(source_data, glob=f\"**/*{ext}\", loader_cls=TextLoader,\n",
    "                                     loader_kwargs={'autodetect_encoding': True})\n",
    "        elif ext in [\".pdf\"]:\n",
    "            loader = DirectoryLoader(source_data, glob=f\"**/*{ext}\", loader_cls=pdf_loader)\n",
    "        else:\n",
    "            continue\n",
    "        docs = loader.load()\n",
    "        all_docs = all_docs + docs\n",
    "\n",
    "    # 数据分片\n",
    "    chunks = get_chunks(all_docs, chunk_size, chunk_overlap)\n",
    "\n",
    "    # 向量数据\n",
    "    text_embeddings = embeddings.embed_documents(chunks)\n",
    "    text_embedding_pairs = list(zip(chunks, text_embeddings))\n",
    "\n",
    "    # 向量数据保存位置\n",
    "    embeddings_save_to = os.path.join(embeddings_data, 'text_embedding_pairs.pickle')\n",
    "\n",
    "    # 保存数据\n",
    "    with open(embeddings_save_to, 'wb') as handle:\n",
    "        pickle.dump(text_embedding_pairs, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    # 存储index.faiss数据\n",
    "    FAISS.from_embeddings(text_embedding_pairs, embeddings).save_local(index_data)\n",
    "\n",
    "    # 压缩文件并保存\n",
    "    index_name = \"vector_base.zip\"\n",
    "    make_archive(\"vector_base\", index_name)\n",
    "\n",
    "    print(\"解析完成，请下载vector_base.zip文件夹到本地使用即可！\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "34e25552116845bf852ec325b3f4598f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "解析完成，请下载vector_base.zip文件夹到本地使用即可！\n"
     ]
    }
   ],
   "source": [
    "# 构造向量数据库\n",
    "path = \"data.zip\"\n",
    "embedding_model_name = TEXT2VEC_LARGE_CHINESE\n",
    "pdf_loader = PyPDFLoader\n",
    "chunk_size=500\n",
    "chunk_overlap=20\n",
    "\n",
    "create_faiss_index_from_zip(\n",
    "    path, embedding_model_name, pdf_loader, chunk_size, chunk_overlap\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
